Run comprehensive AI research cycle with multi-agent orchestration, MCP tool integration, and automated hypothesis generation/validation pipeline

**Usage:** `/ai-research <Research Topic or Technology>`

This command deploys a sophisticated research ecosystem that operates as a true AI research team:

**Multi-Agent Architecture:**
- **Primary Orchestrator** coordinates specialized subagents across research phases
- **Subagents with MCP Tools** leverage Model Context Protocol for:
  - Academic database queries and paper analysis
  - Code repository mining and pattern extraction
  - Experimental infrastructure management
  - Statistical validation and reproducibility checks
  - Knowledge graph construction and traversal

**Research Methodology:**
- **Parallelized Research Streams** - Multiple hypothesis branches explored simultaneously
- **Test-First Development** - Every code artifact begins with validation criteria
- **Continuous Validation Loop** - Test often with automated CI/CD for experiments
- **Iterative Refinement** - Failed experiments spawn new hypothesis branches
- **Cross-Validation** - Subagents independently verify findings before consensus

**Core Capabilities:**
- Literature gap analysis with citation network mapping
- Hypothesis generation with Bayesian confidence scoring
- Parallel experiment execution across distributed compute
- Real-time experiment monitoring and adaptive parameter tuning
- Statistical analysis with p-value correction for multiple comparisons
- Innovation opportunity identification through patent/paper gap analysis
- Automated research artifact generation (papers, code, datasets)
- Reproducibility package creation with containerized environments

**MCP Tool Integration:**
- Direct access to research databases (arXiv, PubMed, IEEE, ACM)
- GPU cluster management for parallel experiments
- Version control integration for experiment tracking
- Jupyter notebook generation with interactive visualizations
- LaTeX document compilation for paper drafts
- Dataset versioning and lineage tracking

The framework implements **true researcher behavior** through:
- Curiosity-driven exploration beyond initial parameters
- Serendipitous discovery logging and pursuit
- Failure analysis that generates new research directions
- Collaborative knowledge synthesis across agent discoveries
- Ethical consideration checks for research implications
- Peer review simulation before finalizing conclusions

**Output Artifacts:**
- Comprehensive research report with interactive visualizations
- Experiment reproduction package
- Annotated bibliography with key insights
- Code repository with full test coverage
- Dataset with documentation
- Draft research paper (if findings are novel)
- Innovation roadmap with commercialization potential

**Example:**  
`/ai-research Context-Aware RoPE`

This would spawn 10+ specialized subagents to explore rotary position embeddings, each with MCP tool access for parallel investigation of mathematical properties, implementation optimizations, benchmark comparisons, and novel applications.