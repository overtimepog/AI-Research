"""
Lightweight framework validation script that checks the structure and logic
without requiring heavy dependencies like PyTorch.
"""

import json
import sys
from pathlib import Path
from dataclasses import asdict


def validate_file_structure():
    """Check that all required files exist."""
    required_files = [
        "experiment_config.py",
        "vocabulary_analysis.py", 
        "bottleneck_experiment.py",
        "metrics.py",
        "README.md"
    ]
    
    missing_files = []
    for file in required_files:
        if not Path(file).exists():
            missing_files.append(file)
    
    if missing_files:
        print(f"âœ— Missing required files: {', '.join(missing_files)}")
        return False
    else:
        print("âœ“ All required files present")
        return True


def validate_config_logic():
    """Validate configuration classes without importing dependencies."""
    try:
        # Read the config file and check for key classes and functions
        with open("experiment_config.py", "r") as f:
            content = f.read()
        
        required_classes = [
            "class ModelConfig",
            "class TaskConfig", 
            "class TrainingConfig",
            "class ExperimentConfig"
        ]
        
        required_configs = [
            "ULTRA_TINY_CONFIG",
            "SCALING_STUDY_CONFIG", 
            "BOTTLENECK_STUDY_CONFIG"
        ]
        
        missing_items = []
        
        for item in required_classes + required_configs:
            if item not in content:
                missing_items.append(item)
        
        if missing_items:
            print(f"âœ— Missing configuration items: {', '.join(missing_items)}")
            return False
        else:
            print("âœ“ Configuration structure validated")
            return True
            
    except Exception as e:
        print(f"âœ— Error validating config: {e}")
        return False


def validate_analysis_structure():
    """Validate vocabulary analysis module structure."""
    try:
        with open("vocabulary_analysis.py", "r") as f:
            content = f.read()
        
        required_classes = [
            "class VocabularySnapshot",
            "class VocabularyAnalyzer"
        ]
        
        required_methods = [
            "def compute_entropy_metrics",
            "def compute_compositionality_metrics",
            "def compute_efficiency_metrics",
            "def generate_analysis_report"
        ]
        
        missing_items = []
        
        for item in required_classes + required_methods:
            if item not in content:
                missing_items.append(item)
        
        if missing_items:
            print(f"âœ— Missing analysis items: {', '.join(missing_items)}")
            return False
        else:
            print("âœ“ Vocabulary analysis structure validated")
            return True
            
    except Exception as e:
        print(f"âœ— Error validating analysis: {e}")
        return False


def validate_metrics_structure():
    """Validate metrics module structure."""
    try:
        with open("metrics.py", "r") as f:
            content = f.read()
        
        required_classes = [
            "class CommunicationMetrics",
            "class BottleneckMetrics",
            "class MetricsAggregator",
            "class MessageBatch"
        ]
        
        required_methods = [
            "def compute_basic_metrics",
            "def compute_entropy_metrics",
            "def compute_efficiency_metrics",
            "def compute_compression_metrics"
        ]
        
        missing_items = []
        
        for item in required_classes + required_methods:
            if item not in content:
                missing_items.append(item)
        
        if missing_items:
            print(f"âœ— Missing metrics items: {', '.join(missing_items)}")
            return False
        else:
            print("âœ“ Metrics structure validated")
            return True
            
    except Exception as e:
        print(f"âœ— Error validating metrics: {e}")
        return False


def validate_experiment_structure():
    """Validate main experiment module structure."""
    try:
        with open("bottleneck_experiment.py", "r") as f:
            content = f.read()
        
        required_classes = [
            "class UltraTinyTransformer",
            "class ReferentialGameEnvironment",
            "class BottleneckExperiment"
        ]
        
        required_methods = [
            "def run_single_experiment",
            "def run_full_experiment",
            "def analyze_aggregate_results"
        ]
        
        missing_items = []
        
        for item in required_classes + required_methods:
            if item not in content:
                missing_items.append(item)
        
        if missing_items:
            print(f"âœ— Missing experiment items: {', '.join(missing_items)}")
            return False
        else:
            print("âœ“ Experiment structure validated")
            return True
            
    except Exception as e:
        print(f"âœ— Error validating experiment: {e}")
        return False


def validate_parameter_logic():
    """Validate parameter scaling logic without dependencies."""
    try:
        # Simple validation of parameter counting logic
        # Estimate parameters for a small model manually
        
        vocab_size = 500
        d_model = 32
        max_seq_len = 64
        n_layers = 2
        n_heads = 2
        d_ff = 64
        
        # Estimate parameters
        embedding_params = vocab_size * d_model  # Token embedding
        pos_embedding_params = max_seq_len * d_model  # Position embedding
        
        # Per layer estimates
        attention_params = 4 * d_model * d_model  # Q, K, V, O projections
        ffn_params = d_model * d_ff + d_ff + d_ff * d_model + d_model  # Two linear layers
        layer_params = attention_params + ffn_params
        
        output_params = d_model * vocab_size  # Output projection
        
        total_params = embedding_params + pos_embedding_params + (layer_params * n_layers) + output_params
        
        print(f"âœ“ Parameter estimation example: {total_params:,} parameters")
        
        # Check if this is in reasonable range for ultra-tiny models
        if 100_000 <= total_params <= 10_000_000:
            print("âœ“ Parameter count in expected ultra-tiny range")
            return True
        else:
            print(f"âš ï¸  Parameter count {total_params:,} outside expected range (100K-10M)")
            return True  # Still pass, just a warning
            
    except Exception as e:
        print(f"âœ— Error validating parameter logic: {e}")
        return False


def generate_usage_examples():
    """Generate usage examples."""
    examples = {
        "ultra_tiny_experiment": '''
# Ultra-Tiny Model Experiment
from experiment_config import ULTRA_TINY_CONFIG
from bottleneck_experiment import BottleneckExperiment

experiment = BottleneckExperiment(ULTRA_TINY_CONFIG)
results = experiment.run_full_experiment()
print(f"Completed {len(results['individual_results'])} variants")
''',
        
        "custom_experiment": '''
# Custom Experiment Configuration
from experiment_config import ExperimentConfig, ModelConfig

config = ExperimentConfig(
    experiment_name="my_experiment",
    parameter_counts=[1_000_000, 5_000_000],
    bottleneck_strengths=[0.2, 0.5],
    model_config=ModelConfig(d_model=32, n_layers=2)
)

experiment = BottleneckExperiment(config)
results = experiment.run_full_experiment()
''',
        
        "vocabulary_analysis": '''
# Vocabulary Analysis
from vocabulary_analysis import VocabularyAnalyzer, VocabularySnapshot

analyzer = VocabularyAnalyzer(vocab_size=1000, max_message_length=8)

# Add snapshots during training
snapshot = VocabularySnapshot(
    step=100,
    messages=[[1, 2, 3], [4, 5], [1, 6]],
    contexts=[{"id": i} for i in range(3)],
    rewards=[0.8, 0.6, 0.9],
    model_params=1000000,
    bottleneck_strength=0.3
)
analyzer.add_snapshot(snapshot)

report = analyzer.generate_analysis_report()
print("Key findings:", report["key_findings"])
''',
        
        "metrics_analysis": '''
# Communication Metrics
from metrics import MessageBatch, CommunicationMetrics, BottleneckMetrics

batch = MessageBatch(
    messages=[[1, 2], [3, 4], [1, 3]],
    contexts=[{"id": i} for i in range(3)],
    rewards=[0.7, 0.8, 0.6]
)

comm_metrics = CommunicationMetrics()
basic = comm_metrics.compute_basic_metrics(batch)
entropy = comm_metrics.compute_entropy_metrics(batch)
efficiency = comm_metrics.compute_efficiency_metrics(batch)

print(f"Success rate: {basic['success_rate']:.2f}")
print(f"Token entropy: {entropy['token_entropy']:.2f}")
print(f"Length efficiency: {efficiency['length_efficiency']:.2f}")
'''
    }
    
    examples_file = Path("usage_examples.py")
    with open(examples_file, "w") as f:
        f.write("# Usage Examples for Ultra-Constrained Vocabulary Framework\n\n")
        for name, code in examples.items():
            f.write(f"# {name.replace('_', ' ').title()}\n")
            f.write(f'"""{name}"""\n')
            f.write(code)
            f.write("\n\n")
    
    print(f"âœ“ Generated usage examples: {examples_file}")
    return True


def main():
    """Run all validation tests."""
    print("=" * 60)
    print("ULTRA-CONSTRAINED VOCABULARY FRAMEWORK VALIDATION")
    print("=" * 60)
    print()
    
    tests = [
        ("File Structure", validate_file_structure),
        ("Configuration Logic", validate_config_logic),
        ("Analysis Structure", validate_analysis_structure),
        ("Metrics Structure", validate_metrics_structure),
        ("Experiment Structure", validate_experiment_structure),
        ("Parameter Logic", validate_parameter_logic),
        ("Usage Examples", generate_usage_examples)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"Testing {test_name}...")
        try:
            if test_func():
                passed += 1
                print(f"âœ“ {test_name} passed\n")
            else:
                print(f"âœ— {test_name} failed\n")
        except Exception as e:
            print(f"âœ— {test_name} failed with exception: {e}\n")
    
    print("=" * 60)
    print("VALIDATION SUMMARY")
    print("=" * 60)
    print(f"Passed: {passed}/{total} tests")
    
    if passed == total:
        print("\nðŸŽ‰ Framework validation successful!")
        print("\nThe framework is structurally sound and ready for use.")
        print("\nTo run experiments, ensure you have the required dependencies:")
        print("  pip install torch numpy pandas scikit-learn matplotlib seaborn scipy networkx")
        print("\nThen run:")
        print("  python bottleneck_experiment.py")
        print("\nOr see usage_examples.py for detailed examples.")
    else:
        print(f"\nâš ï¸  {total - passed} validation(s) failed.")
        print("Please address the issues above before using the framework.")
    
    print("\n" + "=" * 60)
    
    return passed == total


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)